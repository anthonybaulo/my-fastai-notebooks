{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing nb_01. And I just copied and pasted the three lines we used to grab the data, and I am just going to pop them into a fucntion, so we can use it to grab MNIST when we needed.\n",
    "And now that we know about broadcasting let's create a normalization function that takes our tensor x and subtracts the mean and divides it by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_01 import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and standard deviation are not 0 and 1, but we want them to be. So that means that we need to subtract the mean and divide by the standard deviation. \n",
    "\n",
    "But not for the validation set! We do not subtract the validation's set mean and divide it by the standard validation's set standard deviation. Because if we did those two data sets would be on a totally different scale. So if the training set was mainly green frogs, and the validation set was mainly red frogs, then if we normalize with the validation set mean variance, we would end up witht hem both having the same coloration and we would not be able to tell the two them apart. \n",
    "\n",
    "So that is an important thing to remember, when normalizing, is to **always make sure your validation and training set are normalized in the same way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "\n",
    "# NB: Use TRAINING, not validation, mean for validation set\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after doing that our mean is pretty close to zero and our standard deviation is very close to 1. And it would be nice to have somethign to easily check that these are tru so let's create a `test_near_zero` function. \n",
    "\n",
    "And then test that the mean is near zero, and the one minus the standard deviation is near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near_zero(x_train.mean())\n",
    "test_near_zero(1-x_train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and n and m and c. The same as before, so n,m the size of the training set, c the number of activations we eventually need in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is going to have one hidden layer. Normally we would want the final output to have 10 activations, because we would use cross entropy across those 10 activations. But to simplify things for now, we are not going to use cross entropy but mean squared error, which means we are going to have 1 activations. (Which makes no sense from a modelling point of view, but we will fix that later and but just to simplify things for now)\n",
    "\n",
    "So let's create a simple neural net with a single hidden layer and a single output activation which we are going to use, mean squared error.\n",
    "So let's pick a hidden size, so the number of hidden we will make 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need two weight matrices and two bias vectors. They are normal random numbers of size m, the number of columns, 768, by nh, number of hidden. And then w2 is nh by 1.\n",
    "\n",
    "Our inputs of the first layer are mean zero standard deviation of one. We want the inputs of the second layer to also be mean zero standard deviation of one. Well, how are we going to do that? Because if we just grab some normal random numbers, and then we define a function called `lin()` (see below), this is our linear layer, which is `x@w + b`, and then create `t`, which is the activation of the linear layer, with our validation set and our weights and biases, we have a mean of minus 5 and standard deviation of 27. Which is terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified kaiming init / he init\n",
    "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)/math.sqrt(nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near_zero(w1.mean())\n",
    "test_near_zero(w1.std()-1/math.sqrt(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0059), tensor(0.9924))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be ~ (0,1) (mean,std)...\n",
    "x_valid.mean(),x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0059), tensor(0.9562))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#...so should this, because we used kaiming init, which is designed to do this\n",
    "t.mean(),t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I am going to let you work through this at home, but once you look at what actually happens when you multiply those things together and add them up as you do in matrix multiplication, you will see that you will not end up with zero and one. but if you instead divide by square root m (root 768) then it is actually damn good. \n",
    "\n",
    "So this is something which PyTorch calls Kaiming initialization. Named after Kaiming He, who wrote the paper, or was the lead writer of a paper we will look at in a moment. So the weights randn gives you random numbers with a mean of zero and a standard deviation of 1. So if you divide by root m, it will have a mean of zero and a standard deviation of 1 on root m.\n",
    "\n",
    "So in general normal random numbers of mean zero and standard deviation of 1 over root of whatever m/nh will give you an output of 1,0. So this may seem like a minor issue, but as we will see in the next couple of lessons, it is the thing the matters when it comes to training neural nets. It is actually in the last few monthspeople have really been noticing how important this is.\n",
    "\n",
    "There have been things like Fixup Initialization, where these folks actually trained a 10000 layer neural network, with no normalization layers, just by basically doing careful initialization. So people are really spending a lot of time now thinking ok: how we initialize things is really important. And we had a lot of success with things like one cycle training and superconvergence, which is all about what happens in those first few iterations, and it turns out that it is all about initializations. So we will spend a lot of time studying this in depth.\n",
    "\n",
    "So the first thing that I am going to point out, def `lin(x, w, b): return x@w + b` is actually not how our first layer is defined. Our first layer is actually defined like this: `t = relu(lin(x_valid, w1, b1))`. \n",
    "\n",
    "So let's define ReLU. So ReLU is just grab our data and replace any negatives with zero, that is what clamp_min is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a lot of things I could have written this, but if you can do it with something that is a single function in PyTorch, it is almost always faster, because that thing is generally written in C for you. So try to find the thing that is close as to what you want as possible, there's a lot of functions in PyTorch. So that's a good thing of implementing ReLU. And unfortunately, that does not have a mean of zero and standard deviation of 1. Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3770), tensor(0.5577))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#...actually it really should be this!\n",
    "t.mean(),t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n",
    "\n",
    "$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n",
    "\n",
    "This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852), which was also the first paper that claimed \"super-human performance\" on Imagenet (and, most importantly, it introduced resnets!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read section 2.2 of the paper for the full explanation, but if you multiply by the sqrt of 2/m, it gets you much closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaiming init / he init for relu\n",
    "w1 = torch.randn(m,nh)*math.sqrt(2/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0005), tensor(0.0506))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(),w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4396), tensor(0.7536))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(lin(x_valid, w1, b1))\n",
    "t.mean(),t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't give us a very nice mean though, naturally our mean is now half, not zero.\n",
    "\n",
    "I haven't seen anybody talk about this in the literature, but something I was just trying over the last week, is something kind of obvious which is to replace ReLU not just with `x.clamp_min()` but with `x.clamp_min(0.) - 0.5`\n",
    "\n",
    "And in my brief experiments that seems to help. So there's another thing that you can try out and see if it actually helps or if I'm just imagining things. It certainly returns you to the correct mean.\n",
    "\n",
    "So now that we have this formula `w1 = torch.randn(m,nh)*math.sqrt(2/m)`, we can replace it with `init.kaiming_normal()` , because it is the same thing.\n",
    "And let's check that it does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.zeros(m,nh)\n",
    "init.kaiming_normal_(w1, mode='fan_out')\n",
    "t = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0002), tensor(0.0502))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(),w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5469), tensor(0.8255))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(),t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init.kaiming_normal_??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fan_in` preserves the magnitude of the variance n the forward pass.<br>\n",
    "`fan_out` preserves the magnitude of the variance in the backward pass.\n",
    "\n",
    "Basically all it is saying is, are you dividing by root m or root nh.<br> \n",
    "Because if you divide by root m, as you will see in that part of the paper I suggested you read, that will keep the variance at one during the forward pass, but if you use nh, it will give you the right variance in the backward pass.\n",
    "\n",
    "So it is weird that I had to say fan_out because according to the documentation that is for the backward pass to keep the unit variance. So why did I need that?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(m,nh).weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it is because our weight shape is 784 by 50, but if you actually create a linear layer with PyTorch, of the same dimensions, it creates it of 50 by 784, the opposite. So how can that possibly work? These are the kind of things that is useful to dig into, so how is this working?\n",
    "\n",
    "To find out how it is working, you need to look at the source code. So you can either set up VS Code and do that, or you can do it with ?? \n",
    "\n",
    "There's a forward function that calls something called F.linear. In PyTorch, F always refers to the torch.nn.functional module. Because it is used everywhere, they've decided that that is worth a single letter.\n",
    "So torch.nn.functional.linear is what it calls, and let's look at how that is defined:\n",
    "inputmatmul(weight.t()). t means transpose. So now we know in PyTorch a linear layer doesn't just do a matrix product, it does a matrix product with a transpose. So in other words, it is going to turn this into 785 by 50, and then do it. So that is why we had to give it the opposite information when we were trying to do it with our linear layer, which doesn't have transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Linear.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.functional.linear??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the main reason I'm showing you this is to show you how can dig into the PyTorch source code, see exactly what is going on, so when you come across these kid of questions, you want to be able to answer them yourself.\n",
    "\n",
    "Which also then leads to the question: if this is how linear layers can be initialized, what about convolutional layers?\n",
    "\n",
    "What does PyTorch do to convolutional layers? So we can look at torch.nn.Conv2d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Conv2d??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It basically doesn't have any code, just documentation, all of the code gets passed down to something called _ConvNd.\n",
    "And so you need to know how to find these things, so if you go the the bottom, you can find the file name it is in, and so you can see this is actually torch.nn.modules.conv, so you can find torch.nn.modules.conv._ConvNd. \n",
    "\n",
    "So here it is, and how it initializes thing, and it calls `kaiming_uniform`, which is basically the same as kaiming_normal but uniform instead, but it has a special multiplier of `math.sqrt(5)`.\n",
    "\n",
    "And that is not documented anywhere, I have no idea where it comes from and from my experiments, `init.kaiming_uniform_(self.weight, a=math.sqrt(5))` seems to work pretty badly. As you will see.\n",
    "\n",
    "So it is kind of useful to look inside the code so when you write your own code, presumably someone has put math.sqrt(5) there for a reason. Wouldn't it have been nice if they had a url above it with a link to the paper that they're implementing, so that we could see what is going on, so that the next person around could see what the hell you are doing.\n",
    "So this particular thing I have a strong feeling isn't great, as you will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.modules.conv._ConvNd.reset_parameters??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are going to try this thing that is subtracting .5 from our ReLU, so like this is prety cool right, we've already designed our new activation function. Is it great, terrible, I don't know, but it is this kind of level of tweak - when people write papers, this is the normal level, it is a minor change to one line of code, it would be interesting to see how much it helps. But if I use it, you can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if...?\n",
    "def relu(x): return x.clamp_min(0.) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0236), tensor(0.8058))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kaiming init / he init for relu\n",
    "w1 = torch.randn(m,nh)*math.sqrt(2./m )\n",
    "t1 = relu(lin(x_valid, w1, b1))\n",
    "t1.mean(),t1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a mean of 0. And interstingly it helps the variance as well, before it was generally around .7 - .8, now it is generally above .8. So it helps both, which makes sence as to why I think I see these better results.\n",
    "So now we have ReLU, we have a linear, we have init, so we can do a forward pass.\n",
    "And so here it is. \n",
    "\n",
    "So remember in PyTorch, a model can just be a function. And so here's our model, just a function that does one linear layer, one linear layer, one relu and one more linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    l3 = lin(l2, w2, b2)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.01 ms ± 55.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an assert, to make sure the shape seems sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model(x_valid).shape==torch.Size([x_valid.shape[0],1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next thing we need for our forward pass is a loss function, and as I said, we are going to simplify things for now by using MSE. Even although that a dump idea obviously. Our model is returning something of size 10000 by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `squeeze()` to get rid of that trailing (,1), in order to use `mse` with a vector.<br> \n",
    "(Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mse we have to make sure they are floats, so let's convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.8964)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what each line in the forward pass returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7458, -1.4146,  1.0774,  ..., -1.5136, -1.5683, -0.4717],\n",
       "         [ 0.4722, -1.0813, -0.8704,  ..., -0.8512, -1.8047, -0.9888],\n",
       "         [-0.4605, -0.6725,  1.4122,  ...,  2.1583,  0.0054, -1.6397],\n",
       "         ...,\n",
       "         [-2.5006,  1.6314, -0.1732,  ..., -0.8130, -0.9691,  0.1910],\n",
       "         [ 0.2092, -1.0262,  0.8989,  ..., -1.4888, -1.5526, -1.3232],\n",
       "         [-1.5572, -0.7802,  1.4053,  ...,  0.4839, -0.6478, -0.3678]]),\n",
       " torch.Size([50000, 50]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = lin(x_train, w1, b1); l1, l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5000, -0.5000,  0.5774,  ..., -0.5000, -0.5000, -0.5000],\n",
       "         [-0.0278, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "         [-0.5000, -0.5000,  0.9122,  ...,  1.6583, -0.4946, -0.5000],\n",
       "         ...,\n",
       "         [-0.5000,  1.1314, -0.5000,  ..., -0.5000, -0.5000, -0.3090],\n",
       "         [-0.2908, -0.5000,  0.3989,  ..., -0.5000, -0.5000, -0.5000],\n",
       "         [-0.5000, -0.5000,  0.9053,  ..., -0.0161, -0.5000, -0.5000]]),\n",
       " torch.Size([50000, 50]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = relu(l1); l2, l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5796],\n",
       "         [ 0.0443],\n",
       "         [ 0.1886],\n",
       "         ...,\n",
       "         [-0.4149],\n",
       "         [ 0.5629],\n",
       "         [ 0.1317]]), torch.Size([50000, 1]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = lin(l2, w2, b2); out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(29.8964), torch.Size([]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(out, y_train); loss, loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There it is, we've done a forward pass.\n",
    "\n",
    "A forward pass is useless, what we need is a backward pass, because that's the thing that tells us how to update our parameters.\n",
    "\n",
    "\n",
    "So we need gradients. Ok. How much do you want to know about matrix calculus? I don't know, it's up to you. But if you want to know everything about matrix calculus, I can point you to this excellent paper by Terence Parr and Jeremy Howard, which tells you everything about matrix calculus from scratch.\n",
    "\n",
    "https://arxiv.org/abs/1802.01528\n",
    "\n",
    "So this is a few weeks work to get through, but it absolutely asumes nothing at all. Basically Terrence and I felt like: we don't know any of this stuff, let's learn all of it and tell other people. So we wrote it with that in mind, and so this will take you all the way up to know everything that you need for deep learning. You can actually get away with a lot less, but if you're hear, maybe it is worth it. I tell you what you do need to know. \n",
    "\n",
    "**What you do need to know is the chain rule.**\n",
    "\n",
    "We start with some input and we stick it through the first linear layer<br> \n",
    "and then through ReLU<br> \n",
    "and then through the second linear layer<br> \n",
    "and then through mse<br> \n",
    "and that gives us our predictions. \n",
    "\n",
    "Or, to put it another way:<br> \n",
    "We start with x<br> \n",
    "Put it through the function lin1().<br> \n",
    "Take the output of that and we put it through the function relu().<br> \n",
    "Take the output of that through a function lin2()<br> \n",
    "Take the output of that and put it through a function mse().<br> \n",
    "And strictly speaking mse has a second argument, the actual target value.\n",
    "\n",
    "`y_hat = mse(lin2(relu(lin1(x))), y)`\n",
    "\n",
    "So if we simplify things down a bit, we could just say\n",
    "\n",
    "`y = f(u), u = f(x)`  that is like a function of a function.\n",
    "And the derivative is `dy/dx = dy/du * du/dx` That's the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass\n",
    "\n",
    "We will now calculate the gradients and store them in the `.g` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fb pass](images/FB_pass.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to do the chain rule, we are going to have to start with the very last function, the loss function, and get the gradient of the loss with respect to output of previous layer.<br> \n",
    "For the output of the previous layer, the MSE is just input minus target squared, so the derivative of that is just 2 times input minus target, because the derivative of bla squared is two times bla. So that's it.\n",
    "\n",
    "I need to store that gradient somewhere. For the chain rule I need to multiply all these things together. So I store it inside a `.g` attribute of the previous layer. So the input of MSE is the same as the output of the previous layer. And now we can easily refer to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ): \n",
    "    \"\"\"\n",
    "    Calculate gradient of the loss with respect to the output of previous\n",
    "    layer, and store it in the 'g' attribute of inp.\n",
    "    \n",
    "    Args:\n",
    "        inp (rank 2 tensor): output of final linear layer, input to mse funct,\n",
    "                            shape=(50000, 1)(n, num_out)\n",
    "        targ (rank 1 tensor): y labels, shape=(50000)(n)\n",
    "    \"\"\"\n",
    "    \n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU gradient is just the input greather than zero.<br>\n",
    "We need to multiply this by the gradient of the next layer, which remember, we stored away (out.g), so we can just grab it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    \"\"\"\n",
    "    Calculate gradient of relu with respect to the input activations \n",
    "    and store it in the 'g' attribute of inp.\n",
    "    \n",
    "    Args:\n",
    "        inp (rank 2 tensor): output of layer 1 (which was fed into initial relu funciton), \n",
    "                            shape=(n, nh)\n",
    "        out (rank 2 tensor): output after relu function, shape=(n, nh)\n",
    "    \"\"\"\n",
    "\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing for the linear layer. \n",
    "\n",
    "This seems like an insightful shortcut...\n",
    "#### The gradient of a matrix product is the matrix product with the transpose:\n",
    "`inp.g = out.g @ w.t()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    \"\"\"\n",
    "    Caluculate the gradients of matrix multiplication with respect to the input\n",
    "    and store them respectively \n",
    "    \n",
    "    Args (for first layer):\n",
    "        inp (rank 2 tensor): input to linear layer 1 (x_train), shape=(n, m) (50000, 784)\n",
    "        out (rank 2 tensor): output of linear layer 1, shape=(n, nh) (50000, 50)\n",
    "        w (rank 2 tensor): weights, shape=(m, nh) (784, 50)\n",
    "        b (rank 1 tensor): bias, shape=(nh) (50)\n",
    "    \"\"\"\n",
    "    \n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that does the forward pass that we have already seen.\n",
    "\n",
    "And then it goes backward, it calls each of the gradients backwards, in reverse order, because we know we need that for the chain rule.\n",
    "And you can notice that every time we are passing in the result of the forward pass. And it also has access to the gradient of the next layer.\n",
    "This is called backpropagation. So when people say 'backpropagation is not just the chain rule', they are basically lying to you. Backrpropagation is the chain rule, where we just save away all the intermediate calculations, so we don't have to calculate them again.\n",
    "So this is a full forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "    # we don't actually need the loss in backward!\n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    # backward pass:\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing here is this value `loss = mse(out, targ)`, we never actually use it. Because the loss never actually appears in the gradients, I mean, by the weights. You still probably want it to print it out or whatever, but it does not appear in the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig  = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So w1g, w2g now contain all of our gradients, which we are going to use for the optimizer. So let's cheat and use PyTorch autograd to check our results, because PyTorch can do it for us. Let's clone all of our weights and biases. And input. and then turn on requires_grad_() for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad_` is how you take a PyTorch tensor and then turn it into a magical auto gradified tensor. So what it is now going to do is everything that is calculated with test tensor is basically going to keep track of what happened, so it keeps track of the first teps in the forward_and_backward, so it can do the backward. You can write it yourself, you just need to make sure that each time you do an operation, you remember what it is and you can go back through them in reverse order. \n",
    "\n",
    "So now we have done requires_grad_ we can do the forward pass like so, and gives us our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    # we don't actually need the loss in backward!\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, ig )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all of our gradients were correct. So that is pretty interesting, that is an actual neural network, that contains all of the main pieces we are going to need, and we have written all these pieces from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some refactoring, and this is massively, closely stolen from the PyTorch api. And it is intersting, I didn't have it in mind, but when I kept refactoring, I just noticed: i recreated the PyTorch api.\n",
    "\n",
    "Let's take each of our layers, `Relu()` and `Lin()`, and create classes. \n",
    "\n",
    "For the forward, use `__call__`. This means that we can now treat this as a function. So if you call this `class Relu()` as a function, it calls this function `__call__`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)-0.5\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the input, calculate and save the output, and return the output.\n",
    "\n",
    "And then for backward, we will calculate the gradient and save it inside `self.inp.g`.<br> \n",
    "\n",
    "So it is the same as earlier, but the forward and backward are moved into the same class.\n",
    "\n",
    "For a more concrete example, using the earlier model we created:<br>\n",
    "`Relu` would take `l1` as input, and return `l2` as output for the forward pass.<br> \n",
    "When `backward` is called, `l2` will already have a gradient inside `.g`, and it will be used to calculate `l1.g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        # Creating a giant outer product, just to sum it, is inefficient!\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice, the backward pass here in gradient, for linear we don't just want the gradient of the outputs with respect to the inputs `self.inp.g`, we also need the gradient of the output with respect to the weights `self.w.g`, and the output with respect to the biases `self.b.g`.<br>\n",
    "So there's our linear layer, forward and backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have our `Mse()`. In the forward, we save the `inp` and `targ` for later, calculate the `out`.\n",
    "\n",
    "And the `backward` is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with this refactoring, we can create our `Model` class with a list of all of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define `loss = Mse()` , and `__call__`, which goes through all of our layers, and executes `x = l(x)`. \n",
    "\n",
    "* That is the function composition, we just call the function on the result of the previous thing.\n",
    "\n",
    "And then at the end call `self.loss`, and then for `backward` we do the exact opposite. We do `self.loss.backward()` and then we do the reversed layers and call `backward` on each one.\n",
    "\n",
    "And remember the backward passes are going to save the gradient away in `.g` in each respective class instance.\n",
    "\n",
    "We can then create our model and call it, call backward and check that the gradients are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g,b1.g,w2.g,b2.g = [None]*4    # zero out the gradients first\n",
    "\n",
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 ms, sys: 8 ms, total: 88 ms\n",
      "Wall time: 21.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.76 s, sys: 5.54 s, total: 9.3 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our gradients are correct, but it took a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of duplicate code, let's get rid of it. \n",
    "\n",
    "Create a new `Module()` class, which handles the `inp` and `out`.<br> \n",
    "So now we are not going to use `__call__` to implement our forward, but call something called `forward` which we will initially set to `raise Exception('not implemented')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)-0.5\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "        \n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we calculated the derivative of the output of the linear layer with respect to the weights, where we did an unsqueeze and an unsqueeze, we can re-express that with einsum. \n",
    "\n",
    "`self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the code is now neater and executes faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g,b1.g,w2.g,b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 ms, sys: 8 ms, total: 88 ms\n",
      "Wall time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 ms, sys: 208 ms, total: 484 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now this almost looks like PyTorch - and we can see why, why we have to inherit from nn.Module, why we have to define forward, it let's PyTorch factor out all the duplicate stuff. So all we have to do is do the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then once we realised, einsum is exactly the same as inp.t() @ out.g, so we replaced einsum with a matrix product, and that is even faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "        \n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g,b1.g,w2.g,b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 32 ms, total: 128 ms\n",
      "Wall time: 32.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 256 ms, sys: 144 ms, total: 400 ms\n",
      "Wall time: 98.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have basically implemented nn.Linear and nn.Module, so we can use it. And the forward is almost the same speed and the backward pass is about twice as fast, I guess because we are calculating all of the gradients, and they are not calculating all of them, only the ones they need, but it is basically the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear and nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "        self.loss = mse\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x.squeeze(), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 40 ms, total: 164 ms\n",
      "Wall time: 87.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 64 ms, total: 160 ms\n",
      "Wall time: 95.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point we are ready in the next lesson to do a training loop.\n",
    "We have a multilayer fully connected neural network, a rectified network, matrix multiply organised, forward and backward passes nicely refactored out into classes and a module class. So in the next lesson we will see how far we can get, hopefully we will build a high quality fast resnet. And we will also take a very deep dive in optimizers and callbacks and training loops and normalization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_fully_connected.ipynb to exp/nb_02.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 02_fully_connected.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
